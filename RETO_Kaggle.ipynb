{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCVdjQtvWVj"
      },
      "source": [
        "----\n",
        "**Helping Visually Impaired People to Identify Foods**\n",
        "\n",
        "Master Ingeniería de Telecomunicación 2023-2024\n",
        "\n",
        "\n",
        "Autor: Carlos González Gamella 100364132\n",
        "\n",
        "\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3uAQ8-puGiT"
      },
      "source": [
        "**Con el modelo preentrenado se consigue 0.90727 de accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NmXaYIG12d1"
      },
      "source": [
        "Conexión con Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzJScmuB11tJ",
        "outputId": "f0314d59-ec15-4f0b-e63b-082af753e466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# If you want to store some models or results into your drive, you might find these lines of code useful\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/ATVIAV/atviav-2324')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omMzkpS9v241"
      },
      "source": [
        "#Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NH_-0GXNvTdq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')\n",
        "\n",
        "# Itertools\n",
        "import itertools\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch.utils.data import sampler\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "import torch.backends.cudnn as cudnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzdpT-TTv6lA"
      },
      "source": [
        "#1. Importación dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA3ODcrsbQ2Y",
        "outputId": "b689fc64-5555-4eb9-e729-2ab7a7237763"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test',\n",
              " 'train',\n",
              " 'googlenet_state.pth',\n",
              " 'model_ft.pth',\n",
              " 'submission_50.csv',\n",
              " 'submission.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Camino a la carpeta del dataset\n",
        "path_to_dataset = '/content/drive/MyDrive/Colab Notebooks/ATVIAV/atviav-2324'\n",
        "\n",
        "# Listar los archivos/directorios en la carpeta del dataset\n",
        "os.listdir(path_to_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrQ057i2By-U",
        "outputId": "4963e578-8a95-488a-8bc5-ddd57a3d3d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple_pie', 'cheesecake', 'chicken_curry', 'french_fries', 'fried_rice', 'hamburger', 'hot_dog', 'ice_cream', 'omelette', 'pizza', 'sushi']\n"
          ]
        }
      ],
      "source": [
        "# Cargar el conjunto de entrenamiento completo\n",
        "full_train_dataset = datasets.ImageFolder('/content/drive/MyDrive/Colab Notebooks/ATVIAV/atviav-2324/train')\n",
        "clases = full_train_dataset.classes\n",
        "print(clases)\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y validación\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Tamaños de los batches\n",
        "batch_size = 64\n",
        "\n",
        "# DataLoader para el conjunto de entrenamiento\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# DataLoader para el conjunto de validación\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-QnIEZ225rG"
      },
      "source": [
        "#1.1 Transformaciones a las imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NOk55fR3-3m"
      },
      "source": [
        "Para poder sacar unos valores medios para estandarizar las imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYm_HU4s3cfO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Cargar el conjunto de datos sin normalización\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# Asegúrate de aplicar la transformación al cargar el conjunto de datos\n",
        "full_train_dataset = datasets.ImageFolder('/content/drive/MyDrive/Colab Notebooks/ATVIAV/atvaiv-2324/atvaiv_dataset/atvaiv_dataset/train', transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Variables para almacenar sumas y conteos\n",
        "mean = 0.\n",
        "std = 0.\n",
        "nb_samples = 0\n",
        "\n",
        "for data, _ in loader:\n",
        "    # Reajustar tamaño del batch\n",
        "    batch_samples = data.size(0)\n",
        "\n",
        "    # Reajustar datos para calcular la media y la std por canal\n",
        "    data = data.view(batch_samples, data.size(1), -1)\n",
        "    mean += data.mean(2).sum(0)\n",
        "    std += data.std(2).sum(0)\n",
        "    nb_samples += batch_samples\n",
        "\n",
        "# Calcular la media y la std final\n",
        "mean /= nb_samples\n",
        "std /= nb_samples\n",
        "\n",
        "# Imprimir los valores medios y desviaciones estándar por canal\n",
        "print(f'Mean: {mean}')\n",
        "print(f'Std: {std}')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yEBuSlFlZ03s"
      },
      "outputs": [],
      "source": [
        "train_transforms = T.Compose([\n",
        "    # Recortar la imagen de manera aleatoria y luego redimensionar a 224x224\n",
        "    T.RandomResizedCrop(224),\n",
        "    # Aplicar volteo horizontal y vertical de manera aleatoria\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomVerticalFlip(p=0.5),\n",
        "    # Rotar la imagen un ángulo aleatorio entre 30 y 70 grados\n",
        "    T.RandomRotation(degrees=(30, 70)),\n",
        "    # Convertir la imagen a tensor\n",
        "    T.ToTensor(),\n",
        "    # Normalizar con los valores medios y desviaciones estándar de los datos\n",
        "    T.Normalize((0.5600, 0.4481, 0.3405), (0.2248, 0.2356, 0.2332))\n",
        "])\n",
        "\n",
        "# Aplicar las transformaciones al conjunto de entrenamiento y test\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "val_dataset.dataset.transform = train_transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJowSbDZ6yNu"
      },
      "source": [
        "# 1.2 Modelo from scracth basado en AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK0KmCWe9HhW",
        "outputId": "fdfcefb7-636a-4fd2-bcbc-dc92f4b1bc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size: torch.Size([64, 11])\n"
          ]
        }
      ],
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0]  # Lee N, C, H, W\n",
        "    return x.view(N, -1)  # \"Aplana\" los valores C * H * W en un único vector por imagen\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, in_channel, num_classes):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.c1 = nn.Conv2d(in_channel, 96, kernel_size=11, stride=4, padding=2)\n",
        "        self.m1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.c2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
        "        self.m2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.c3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
        "        self.c4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n",
        "        self.c5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
        "        self.m3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fcout = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.c1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.m1(out)\n",
        "\n",
        "        out = self.c2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.m2(out)\n",
        "\n",
        "        out = self.c3(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.c4(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.c5(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.m3(out)\n",
        "\n",
        "        out = flatten(out)\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.fcout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def test_AlexNet():\n",
        "    x = torch.randn((64, 3, 224, 224))  # minibatch size 64, image size [3, 224, 224]\n",
        "    model = AlexNet(in_channel=3, num_classes=11)\n",
        "    scores = model(x)\n",
        "    print(\"Output size:\", scores.size())\n",
        "\n",
        "test_AlexNet()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUs3VL1mGwEh"
      },
      "source": [
        "# Entrenando el modelo scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJfu2IXIHwfU"
      },
      "source": [
        "Para evaluar sobre los datos de eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P4YaHQLCickb"
      },
      "outputs": [],
      "source": [
        "def eval_model(loader, model, device = torch.device('cuda')):\n",
        "    loss = []\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad(): #normalmente uso modelo de descenso por gradiente, haciendo eso hago que no modifique ningún peso\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x) # saca las probabilidades de cada clase\n",
        "            loss.append(F.cross_entropy(scores, y).data.cpu().numpy())\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        mean_loss = np.mean(loss)\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "        print('Loss=%.2f' % mean_loss)\n",
        "    return acc, mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HslLl9YORJAJ"
      },
      "outputs": [],
      "source": [
        "def training_loop_plot(model, optimizer, epochs=1, device = torch.device('cuda')):\n",
        "    ##############################################################\n",
        "    # - - - WRITE YOUR CODE HERE - - -\n",
        "    # 1) Modify the function 'training_loop' to represent,\n",
        "    # using Matplotlib, the value of the loss for the training\n",
        "    # and validation sets over epochs.\n",
        "\n",
        "    ##############################################################\n",
        "    loss_val_list = []\n",
        "    loss_train_list = []\n",
        "\n",
        "    meanloss_val_list = []\n",
        "    meanloss_train_list = []\n",
        "\n",
        "    model.to(device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(train_loader):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = nn.CrossEntropyLoss()\n",
        "            loss = loss(scores, y) ### TO COMPLETE [COMPLETED]\n",
        "\n",
        "            loss_train_list.append(loss.item())\n",
        "\n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "\n",
        "        # Save mean_loss values for each epoch\n",
        "        meanloss_train_list.append(np.mean(loss_train_list))\n",
        "\n",
        "        acc, mean_loss = eval_model(val_loader, model)\n",
        "        meanloss_val_list.append(mean_loss)\n",
        "\n",
        "    # Plot mean loss for each epoch and set\n",
        "    plt.plot(loss_val_list)\n",
        "    plt.plot(meanloss_train_list, label='Train')\n",
        "    plt.plot(meanloss_val_list, label='Validation')\n",
        "    plt.legend(title = 'Data')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.title(\"Mean Loss over Epochs\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qASoSpHsH5wQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "ddb62700-2faa-4d45-f551-b4a1c2920951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando con tasa de aprendizaje: 0.015\n",
            "Got 126 / 1315 correct (9.58)\n",
            "Loss=2.40\n",
            "Got 163 / 1315 correct (12.40)\n",
            "Loss=2.40\n",
            "Got 160 / 1315 correct (12.17)\n",
            "Loss=2.40\n",
            "Got 153 / 1315 correct (11.63)\n",
            "Loss=2.40\n",
            "Got 161 / 1315 correct (12.24)\n",
            "Loss=2.40\n",
            "Got 194 / 1315 correct (14.75)\n",
            "Loss=2.39\n",
            "Got 178 / 1315 correct (13.54)\n",
            "Loss=2.37\n",
            "Got 200 / 1315 correct (15.21)\n",
            "Loss=2.31\n",
            "Got 163 / 1315 correct (12.40)\n",
            "Loss=2.38\n",
            "Got 184 / 1315 correct (13.99)\n",
            "Loss=2.37\n",
            "Got 194 / 1315 correct (14.75)\n",
            "Loss=2.31\n",
            "Got 200 / 1315 correct (15.21)\n",
            "Loss=2.28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Llamada a la función de entrenamiento con diferentes tasas de aprendizaje\n",
        "lr = 0.015\n",
        "num_epochs = 12\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instanciando el modelo para cada tasa de aprendizaje\n",
        "model_AlexNet = AlexNet(in_channel=3, num_classes=11).to(device)\n",
        "\n",
        "# Definición del optimizador con la tasa de aprendizaje actual\n",
        "optimizer = optim.SGD(model_AlexNet.parameters(), lr=lr)\n",
        "\n",
        "# Llamada a la función de entrenamiento\n",
        "print(f\"Entrenando con tasa de aprendizaje: {lr}\")\n",
        "training_loop_plot(model_AlexNet, optimizer, num_epochs, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDw7vD0-zZtO"
      },
      "source": [
        "# Evaluando el modelo sobre los datos de test\n",
        "Aplico las mismas transformaciones que al conjunto de datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QOAYez8vin9j"
      },
      "outputs": [],
      "source": [
        "#tranformaciones de las imagenes de test\n",
        "transform_test = T.Compose([\n",
        "    # Recortar la imagen de manera aleatoria y luego redimensionar a 224x224\n",
        "    T.RandomResizedCrop(224),\n",
        "\n",
        "    # Aplicar volteo horizontal y vertical de manera aleatoria\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomVerticalFlip(p=0.5),\n",
        "    # Rotar la imagen un ángulo aleatorio entre 30 y 70 grados\n",
        "    T.RandomRotation(degrees=(30, 70)),\n",
        "    # Convertir la imagen a tensor\n",
        "    T.ToTensor(),\n",
        "    # Normalizar con los valores medios y desviaciones estándar de los datos\n",
        "    T.Normalize((0.5600, 0.4481, 0.3405), (0.2248, 0.2356, 0.2332))\n",
        "])\n",
        "\n",
        "#funcion para crear un Dataset de los datos de test que no estan etiquetados\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.ids = sorted([int(file[0:-4]) for file in os.listdir(img_dir)])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.img_dir + '/' + str(self.ids[idx]) + '.jpg'\n",
        "        image = Image.open(path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def etiq_model(loader, model, device):\n",
        "    model.eval()  # Configurar el modelo en modo de evaluación\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for x in loader:\n",
        "            x = x.to(device=device, dtype=torch.float32)\n",
        "            scores = model(x)\n",
        "            _, batch_preds = scores.max(1)\n",
        "            preds.extend(batch_preds.cpu().numpy())  # Acumular las predicciones\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1rdkS9PwnNlN"
      },
      "outputs": [],
      "source": [
        "test_dataset = ImageDataset(img_dir='/content/drive/MyDrive/Colab Notebooks/ATVIAV/atviav-2324/test',transform=transform_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxCngRWqGnoy"
      },
      "outputs": [],
      "source": [
        "# Configurar el dispositivo\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_AlexNet.to(device)\n",
        "\n",
        "# Obtener las predicciones\n",
        "preds = etiq_model(test_loader, model_AlexNet, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CStHqjBnHCzm"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_dataset.ids,\n",
        "    'Predicted': preds\n",
        "})\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GWXgD10HHUY"
      },
      "source": [
        "Guardando fichero a subir a KAGGLE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D4BtyQuoHEG8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39a45a03-f68d-41cc-8d12-21bfda1a12d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7501e465-3c40-4f23-af7a-aba254a61d7f\", \"submission.csv\", 9929)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VhKPH09ITD"
      },
      "source": [
        "# 2. MODELOS PREENTRENADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UyvVMjycK1Fu"
      },
      "outputs": [],
      "source": [
        "train_transforms = T.Compose([\n",
        "    # Recortar la imagen de manera aleatoria y luego redimensionar a 224x224\n",
        "    T.RandomResizedCrop(224),\n",
        "    # Aplicar volteo horizontal y vertical de manera aleatoria\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomVerticalFlip(p=0.5),\n",
        "    # Rotar la imagen un ángulo aleatorio entre 30 y 70 grados\n",
        "    T.RandomRotation(degrees=(30, 70)),\n",
        "    # Ajusta el brillo, el contraste, la saturación y el tono de manera aleatoria\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    # Convertir la imagen a tensor\n",
        "    T.ToTensor(),\n",
        "    # Normalizar con los valores medios y desviaciones estándar del full_dataset\n",
        "    T.Normalize((0.5600, 0.4481, 0.3405), (0.2248, 0.2356, 0.2332))\n",
        "])\n",
        "val_transforms = T.Compose([\n",
        "    # Recortar la imagen de manera aleatoria y luego redimensionar a 224x224\n",
        "    T.RandomResizedCrop(224),\n",
        "    # Convertir la imagen a tensor\n",
        "    T.ToTensor(),\n",
        "    # Normalizar con los valores medios y desviaciones estándar del full_dataset\n",
        "    T.Normalize((0.5600, 0.4481, 0.3405), (0.2248, 0.2356, 0.2332))\n",
        "])\n",
        "\n",
        "# Aplicar las transformaciones al conjunto de entrenamiento y validación\n",
        "train_dataset.dataset.transform = train_transforms\n",
        "val_dataset.dataset.transform = val_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-CO7z9AHW-9"
      },
      "source": [
        "#Entrenamos el modelo con los datos de train y validamos para ver los resultados obtenidos.\n",
        "#Usamos el transformer vit_b_16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Q9vDMoeVumL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c71b615-1a7f-45c6-e75f-8b289ef61a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 139MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=1000, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Entreno el modelo con los datos de train\n",
        "model_ft = models.vit_b_16(pretrained=True)\n",
        "num_ftrs = model_ft.num_classes\n",
        "model_ft.fc = nn.Linear(num_ftrs, 11)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Mover el modelo al dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nihST5fy5Pp2"
      },
      "outputs": [],
      "source": [
        "# Ajustar la tasa de aprendizaje inicial si es necesario\n",
        "optimizer = optim.Adam(model_ft.parameters(), lr=0.0001)\n",
        "# Usar un programador de tasa de aprendizaje\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.05)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YmprnnSPWl5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca35c2e-e677-431d-fb1e-664dda60dcbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1/15, Pérdida: 1.2730564054954483\n",
            "Precisión en el conjunto de validación: 78.85931558935361%\n",
            "Época 2/15, Pérdida: 0.5082883719938347\n",
            "Precisión en el conjunto de validación: 80.22813688212928%\n",
            "Época 3/15, Pérdida: 0.40183511421263934\n",
            "Precisión en el conjunto de validación: 84.18250950570342%\n",
            "Época 4/15, Pérdida: 0.3560065468212208\n",
            "Precisión en el conjunto de validación: 83.95437262357414%\n",
            "Época 5/15, Pérdida: 0.2941024025161582\n",
            "Precisión en el conjunto de validación: 86.31178707224335%\n",
            "Época 6/15, Pérdida: 0.30793624505939254\n",
            "Precisión en el conjunto de validación: 83.8022813688213%\n",
            "Época 7/15, Pérdida: 0.24829727572848997\n",
            "Precisión en el conjunto de validación: 83.8022813688213%\n",
            "Época 8/15, Pérdida: 0.17634002249463496\n",
            "Precisión en el conjunto de validación: 86.08365019011407%\n",
            "Época 9/15, Pérdida: 0.13880248373396784\n",
            "Precisión en el conjunto de validación: 87.68060836501901%\n",
            "Época 10/15, Pérdida: 0.11685153229319188\n",
            "Precisión en el conjunto de validación: 87.90874524714829%\n",
            "Época 11/15, Pérdida: 0.12382859575102129\n",
            "Precisión en el conjunto de validación: 86.92015209125475%\n",
            "Época 12/15, Pérdida: 0.12188098584133458\n",
            "Precisión en el conjunto de validación: 86.99619771863118%\n",
            "Época 13/15, Pérdida: 0.10830134828855474\n",
            "Precisión en el conjunto de validación: 86.69201520912547%\n",
            "Época 14/15, Pérdida: 0.11716362488377526\n",
            "Precisión en el conjunto de validación: 87.45247148288973%\n",
            "Época 15/15, Pérdida: 0.1029460786740823\n",
            "Precisión en el conjunto de validación: 88.212927756654%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 15\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_ft.train()  # Configurar en modo de entrenamiento\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_ft(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Época {epoch+1}/{num_epochs}, Pérdida: {running_loss/len(train_loader)}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validación\n",
        "    model_ft.eval()  # Configurar en modo de evaluación\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model_ft(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Precisión en el conjunto de validación: {accuracy}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYUY1XjUn5pe"
      },
      "source": [
        "Para guardar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ynHg0J8ziI1L"
      },
      "outputs": [],
      "source": [
        "torch.save(model_ft.state_dict(), 'model_ft.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1MB2rAln9V8"
      },
      "source": [
        "Para cargar el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FOo-J8Qyhclw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf0b0f4-ce99-4ca8-8485-b9e9e4d836bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=1000, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_ft.load_state_dict(torch.load('model_ft.pth'))\n",
        "model_ft.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sNC2CaLFLaPw"
      },
      "outputs": [],
      "source": [
        "#tranformaciones de las imagenes de test\n",
        "transform_test = T.Compose([\n",
        "    # Recortar la imagen de manera aleatoria y luego redimensionar a 224x224\n",
        "    T.RandomResizedCrop(224),\n",
        "    T.ToTensor(),\n",
        "    # Normalizar con los valores medios y desviaciones estándar del full_dataset\n",
        "    T.Normalize((0.5600, 0.4481, 0.3405), (0.2248, 0.2356, 0.2332))])\n",
        "\n",
        "#funcion para crear un Dataset de los datos de test que no estan etiquetados\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.ids = sorted([int(file[0:-4]) for file in os.listdir(img_dir)])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.img_dir + '/' + str(self.ids[idx]) + '.jpg'\n",
        "        image = Image.open(path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def etiq_model(loader, model, device):\n",
        "    model.eval()  # Configurar el modelo en modo de evaluación\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for x in loader:\n",
        "            x = x.to(device=device, dtype=torch.float32)\n",
        "            scores = model(x)\n",
        "            _, batch_preds = scores.max(1)\n",
        "            preds.extend(batch_preds.cpu().numpy())  # Acumular las predicciones\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OAzA8G9roNF7"
      },
      "outputs": [],
      "source": [
        "#Aplico la misma normalización que uso en el conjunto de datos de val\n",
        "test_dataset = ImageDataset(img_dir='/content/drive/MyDrive/Colab Notebooks/ATVIAV/atviav-2324/test',transform=transform_test)\n",
        "batch_size = 64 #numero total de imagenes de test\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Configurar el dispositivo\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft.to(device)  # Asegúrate de que el modelo esté en el dispositivo correcto\n",
        "\n",
        "# Obtener las predicciones\n",
        "preds = etiq_model(test_loader, model_ft, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "13FppdUhirMx"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'Id': test_dataset.ids,\n",
        "    'Predicted': preds\n",
        "})\n",
        "submission_df.to_csv('submission_50.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGgKgFa2oAk9"
      },
      "source": [
        "Guardando fichero a subir a KAGGLE:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qOfn0MYXkWJG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f936b275-efdf-4ed1-a2eb-5426b026536a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_202b8fe3-84dc-4019-b1d6-35af9b5977e9\", \"submission_50.csv\", 9530)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('submission_50.csv')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}